{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vciaxf7Fyuse"
      },
      "source": [
        "# Linear Regression and Gradient Descent\n",
        "\n",
        "## Linear Regression Model\n",
        "\n",
        "The linear regression model is defined as:\n",
        "\n",
        "$f_{w,b}(x) = wx + b$\n",
        "\n",
        "Where:\n",
        "- $w$ is the weight (slope)\n",
        "- $b$ is the bias (y-intercept)\n",
        "- $x$ is the input variable\n",
        "\n",
        "## Cost Function\n",
        "\n",
        "The cost function $J(w,b)$ is given by:\n",
        "\n",
        "$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2$\n",
        "\n",
        "Where:\n",
        "- $m$ is the number of training examples\n",
        "- $x^{(i)}$ is the i-th input\n",
        "- $y^{(i)}$ is the i-th target output\n",
        "\n",
        "## Gradient Descent Algorithm\n",
        "\n",
        "The gradient descent algorithm is used to minimize the cost function:\n",
        "\n",
        "\n",
        "w = w - α $\\frac{\\partial}{\\partial w} J(w,b)$\n",
        "\n",
        "b = b - α $\\frac{\\partial}{\\partial b}J(w,b)$\n",
        "\n",
        "\n",
        "Where:\n",
        "- α is the learning rate\n",
        "- ∂/∂w J(w,b) is the partial derivative of J with respect to w\n",
        "- ∂/∂b J(w,b) is the partial derivative of J with respect to b\n",
        "\n",
        "The partial derivatives are calculated as:\n",
        "\n",
        "$\\frac{\\partial}{\\partial w} J(w,b) = \\frac{1}{m} \\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial}{\\partial b} J(w,b) = \\frac{1}{m} \\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})$\n",
        "\n",
        "These formulas are used to update the parameters w and b in each iteration of the gradient descent algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "32CPA-r0dkeO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "e5AI5gmEynaF"
      },
      "outputs": [],
      "source": [
        "class MultipleLinearRegression:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Initializes the Multiple Linear Regression model with training data.\n",
        "\n",
        "        Parameters:\n",
        "        - x_train: Matrix of input features.\n",
        "        - y_train: Array of corresponding target values.\n",
        "        \"\"\"\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.b_in = 0\n",
        "        self.m = len(y_train)  # Number of training examples\n",
        "        self.w_in = np.zeros(len(self.x_train[0]))  # Initialize the weight with zeros\n",
        "\n",
        "    def cost_function(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the cost function for multiple linear regression, which measures the difference between\n",
        "        predicted values and actual values.\n",
        "\n",
        "        The cost function used here is Mean Squared Error (MSE).\n",
        "\n",
        "        Parameters:\n",
        "        - w: Weight matrix for the multiple linear regression model.\n",
        "        - b: Bias for the linear regression model.\n",
        "\n",
        "        Returns:\n",
        "        - The computed cost (MSE).\n",
        "        \"\"\"\n",
        "        total_cost = 0\n",
        "\n",
        "        # Sum of squared differences between predicted and actual values\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            total_cost += (y_predicted - self.y_train[i]) ** 2\n",
        "\n",
        "        # Return the mean of the squared differences\n",
        "        return (1 / (2 * self.m)) * total_cost\n",
        "\n",
        "    def compute_prediction_array(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the predicted values (y_hat) for the training set using the current weight and bias.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Weight matrix for the multiple linear regression model.\n",
        "        - b: Bias for the linear regression model.\n",
        "\n",
        "        Returns:\n",
        "        - An array of predicted values for each training example.\n",
        "        \"\"\"\n",
        "        result = np.zeros(self.m)\n",
        "        for i in range(self.m):\n",
        "            result[i] = np.dot(self.x_train[i], w) + b\n",
        "\n",
        "        return result\n",
        "\n",
        "    def gradient_w(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cost function with respect to the weight matrix(w).\n",
        "        This gradient indicates how much the cost would change with a small change in the weight.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Current weight matrix value.\n",
        "        - b: Current bias value.\n",
        "\n",
        "        Returns:\n",
        "        - The gradient vector for the weight matrix.\n",
        "        \"\"\"\n",
        "        gradient_value = np.zeros(len(self.x_train[0]))\n",
        "\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            for j in range(len(self.x_train[0])):  \n",
        "                gradient_value[j] += (y_predicted - self.y_train[i]) * self.x_train[i][j]\n",
        "                \n",
        "        # Average gradient over all training examples\n",
        "        return (1 / self.m) * gradient_value\n",
        "\n",
        "    def gradient_b(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cost function with respect to the bias (b).\n",
        "        This gradient indicates how much the cost would change with a small change in the bias.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Current weight value.\n",
        "        - b: Current bias value.\n",
        "\n",
        "        Returns:\n",
        "        - The gradient value for the bias.\n",
        "        \"\"\"\n",
        "        gradient_value = 0\n",
        "\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            gradient_value += y_predicted - self.y_train[i]\n",
        "\n",
        "        # Average gradient over all training examples\n",
        "        return (1 / self.m) * gradient_value\n",
        "\n",
        "    def gradient_descent(self, iterations=1200, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Performs the gradient descent optimization to find the optimal values of weight matrix(w) and bias (b)\n",
        "        that minimize the cost function.\n",
        "\n",
        "        Parameters:\n",
        "        - iterations: Number of iterations to run the gradient descent (default is 1200).\n",
        "        - learning_rate: Step size for each iteration of gradient descent (default is 0.01).\n",
        "\n",
        "        Returns:\n",
        "        - Optimized weight matrix (w_optimised) and bias (b_optimised).\n",
        "        \"\"\"\n",
        "        w_optimised = copy.deepcopy(self.w_in)  # Initialize the optimized weight matrix with the initial value\n",
        "                                                #avoid modifying global w within function, so use deepcopy\n",
        "        b_optimised = self.b_in  # Initialize the optimized bias with the initial value\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # Compute gradients for weight matrix and bias\n",
        "            dj_dw = self.gradient_w(w_optimised, b_optimised)\n",
        "            dj_db = self.gradient_b(w_optimised, b_optimised)\n",
        "\n",
        "            # Update the weight and bias by moving in the opposite direction of the gradients\n",
        "            temp_w = w_optimised - learning_rate * dj_dw\n",
        "            temp_b = b_optimised - learning_rate * dj_db\n",
        "\n",
        "            # Assign the updated values back to the optimized variables\n",
        "            w_optimised = temp_w\n",
        "            b_optimised = temp_b\n",
        "\n",
        "            # Print the progress every 100 iterations\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"After {i + 1} iterations: w -> {w_optimised}      b -> {b_optimised}.       cost -> {self.cost_function(w_optimised, b_optimised)}\")\n",
        "\n",
        "        return w_optimised, b_optimised\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "YlWKWULVjgNv",
        "outputId": "adc1128d-8040-4902-90d2-eea5ab67afe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After 100 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 200 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 300 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 400 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 500 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 600 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 700 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 800 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 900 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n",
            "After 1000 iterations: w -> [nan nan nan nan]      b -> nan.       cost -> nan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1216/805601188.py:74: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  gradient_value[j] += (y_predicted - self.y_train[i]) * self.x_train[i][j]\n",
            "/tmp/ipykernel_1216/805601188.py:121: RuntimeWarning: invalid value encountered in subtract\n",
            "  temp_w = w_optimised - learning_rate * dj_dw\n"
          ]
        }
      ],
      "source": [
        "# Training data\n",
        "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "model = MultipleLinearRegression(x_train, y_train)\n",
        "\n",
        "\n",
        "# Perform gradient descent to find the optimal weight and bias\n",
        "w, b = model.gradient_descent(iterations=1000, learning_rate=0.001)\n",
        "\n",
        "# # Scatter plot of the training data\n",
        "# plt.scatter(x_train, y_train, marker = \"x\", color = \"red\")\n",
        "\n",
        "# # Display the final optimized weight and bias\n",
        "# print(f\"w -> {w}.     b -> {b}\")\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UewdIci2yr47"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2104    5    1   45]\n"
          ]
        }
      ],
      "source": [
        "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])\n",
        "\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
