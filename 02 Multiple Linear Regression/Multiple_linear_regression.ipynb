{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vciaxf7Fyuse"
      },
      "source": [
        "# Multiple Linear Regression and Gradient Descent\n",
        "\n",
        "## Multiple Linear Regression Model\n",
        "\n",
        "The multiple linear regression model is defined as:\n",
        "\n",
        "$$\n",
        "f_{w,b}(x) = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = \\sum_{j=1}^{n} w_jx_j + b\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $w_1, w_2, \\dots, w_n$ are the weights (coefficients) for each feature.\n",
        "- $b$ is the bias (y-intercept).\n",
        "- $x_1, x_2, \\dots, x_n$ are the input features.\n",
        "\n",
        "## Cost Function\n",
        "\n",
        "The cost function $J(w,b)$ for multiple linear regression is given by:\n",
        "\n",
        "$$\n",
        "J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $m$ is the number of training examples.\n",
        "- $x^{(i)} = [x_1^{(i)}, x_2^{(i)}, \\dots, x_n^{(i)}]$ is the feature vector for the $i$-th training example.\n",
        "- $y^{(i)}$ is the $i$-th target output.\n",
        "\n",
        "## Gradient Descent Algorithm\n",
        "\n",
        "The gradient descent algorithm is used to minimize the cost function:\n",
        "\n",
        "$$\n",
        "w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(w,b) \\quad \\text{for } j = 1, 2, \\dots, n\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = b - \\alpha \\frac{\\partial}{\\partial b} J(w,b)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $\\frac{\\partial}{\\partial w_j} J(w,b)$ is the partial derivative of $J$ with respect to $w_j$.\n",
        "- $\\frac{\\partial}{\\partial b} J(w,b)$ is the partial derivative of $J$ with respect to $b$.\n",
        "\n",
        "The partial derivatives are calculated as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial w_j} J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial b} J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{w,b}(x^{(i)}) - y^{(i)} \\right)\n",
        "$$\n",
        "\n",
        "These formulas are used to update the parameters $w_j$ and $b$ in each iteration of the gradient descent algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "32CPA-r0dkeO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy, math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "e5AI5gmEynaF"
      },
      "outputs": [],
      "source": [
        "class MultipleLinearRegression:\n",
        "    def __init__(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Initializes the Multiple Linear Regression model with training data.\n",
        "\n",
        "        Parameters:\n",
        "        - x_train: Matrix of input features.\n",
        "        - y_train: Array of corresponding target values.\n",
        "        \"\"\"\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.b_in = 0\n",
        "        self.m = len(y_train)  # Number of training examples\n",
        "        self.w_in = np.zeros(len(self.x_train[0]))  # Initialize the weight with zeros\n",
        "\n",
        "    def cost_function(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the cost function for multiple linear regression, which measures the difference between\n",
        "        predicted values and actual values.\n",
        "\n",
        "        The cost function used here is Mean Squared Error (MSE).\n",
        "\n",
        "        Parameters:\n",
        "        - w: Weight matrix for the multiple linear regression model.\n",
        "        - b: Bias for the linear regression model.\n",
        "\n",
        "        Returns:\n",
        "        - The computed cost (MSE).\n",
        "        \"\"\"\n",
        "        total_cost = 0\n",
        "\n",
        "        # Sum of squared differences between predicted and actual values\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            total_cost += (y_predicted - self.y_train[i]) ** 2\n",
        "\n",
        "        # Return the mean of the squared differences\n",
        "        return (1 / (2 * self.m)) * total_cost\n",
        "\n",
        "    def compute_prediction_array(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the predicted values (y_hat) for the training set using the current weight and bias.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Weight matrix for the multiple linear regression model.\n",
        "        - b: Bias for the linear regression model.\n",
        "\n",
        "        Returns:\n",
        "        - An array of predicted values for each training example.\n",
        "        \"\"\"\n",
        "        result = np.zeros(self.m)\n",
        "        for i in range(self.m):\n",
        "            result[i] = np.dot(self.x_train[i], w) + b\n",
        "\n",
        "        return result\n",
        "\n",
        "    def gradient_w(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cost function with respect to the weight matrix(w).\n",
        "        This gradient indicates how much the cost would change with a small change in the weight.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Current weight matrix value.\n",
        "        - b: Current bias value.\n",
        "\n",
        "        Returns:\n",
        "        - The gradient vector for the weight matrix.\n",
        "        \"\"\"\n",
        "        gradient_value = np.zeros(len(self.x_train[0]))\n",
        "\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            for j in range(len(self.x_train[0])):  \n",
        "                gradient_value[j] += (y_predicted - self.y_train[i]) * self.x_train[i][j]\n",
        "                \n",
        "        # Average gradient over all training examples\n",
        "        return (1 / self.m) * gradient_value\n",
        "\n",
        "    def gradient_b(self, w, b):\n",
        "        \"\"\"\n",
        "        Computes the gradient of the cost function with respect to the bias (b).\n",
        "        This gradient indicates how much the cost would change with a small change in the bias.\n",
        "\n",
        "        Parameters:\n",
        "        - w: Current weight value.\n",
        "        - b: Current bias value.\n",
        "\n",
        "        Returns:\n",
        "        - The gradient value for the bias.\n",
        "        \"\"\"\n",
        "        gradient_value = 0\n",
        "\n",
        "        for i in range(self.m):\n",
        "            y_predicted = np.dot(self.x_train[i], w) + b\n",
        "            gradient_value += y_predicted - self.y_train[i]\n",
        "\n",
        "        # Average gradient over all training examples\n",
        "        return (1 / self.m) * gradient_value\n",
        "\n",
        "    def gradient_descent(self, iterations=1200, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        Performs the gradient descent optimization to find the optimal values of weight matrix(w) and bias (b)\n",
        "        that minimize the cost function.\n",
        "\n",
        "        Parameters:\n",
        "        - iterations: Number of iterations to run the gradient descent (default is 1200).\n",
        "        - learning_rate: Step size for each iteration of gradient descent (default is 0.01).\n",
        "\n",
        "        Returns:\n",
        "        - Optimized weight matrix (w_optimised) and bias (b_optimised).\n",
        "        \"\"\"\n",
        "        w_optimised = copy.deepcopy(self.w_in)  # Initialize the optimized weight matrix with the initial value\n",
        "                                                #avoid modifying global w within function, so use deepcopy\n",
        "        b_optimised = self.b_in  # Initialize the optimized bias with the initial value\n",
        "\n",
        "        for i in range(iterations):\n",
        "            # Compute gradients for weight matrix and bias\n",
        "            dj_dw = self.gradient_w(w_optimised, b_optimised)\n",
        "            dj_db = self.gradient_b(w_optimised, b_optimised)\n",
        "\n",
        "            # Update the weight and bias by moving in the opposite direction of the gradients\n",
        "            temp_w = w_optimised - learning_rate * dj_dw\n",
        "            temp_b = b_optimised - learning_rate * dj_db\n",
        "\n",
        "            # Assign the updated values back to the optimized variables\n",
        "            w_optimised = temp_w\n",
        "            b_optimised = temp_b\n",
        "\n",
        "            # Print the progress every 100 iterations\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"After {i + 1} iterations: w -> {w_optimised}      b -> {b_optimised}.       cost -> {self.cost_function(w_optimised, b_optimised)}\")\n",
        "\n",
        "        return w_optimised, b_optimised\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "YlWKWULVjgNv",
        "outputId": "adc1128d-8040-4902-90d2-eea5ab67afe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After 10 iterations: w -> [2.02184672e-01 4.98191696e-04 4.85540318e-05 4.39075551e-03]      b -> 9.93447426134968e-05.       cost -> 696.9718639233548\n",
            "After 20 iterations: w -> [ 2.02203074e-01  5.31142534e-04 -6.63628987e-05  3.66547540e-03]      b -> 7.521019197807132e-05.       cost -> 696.8636174971623\n",
            "After 30 iterations: w -> [ 2.02221453e-01  5.64091012e-04 -1.81263214e-04  2.94051918e-03]      b -> 5.108704351800476e-05.       cost -> 696.7554660853489\n",
            "After 40 iterations: w -> [ 0.20223982  0.00059704 -0.00029615  0.00221589]      b -> 2.6975301357397917e-05.       cost -> 696.6474096031822\n",
            "After 50 iterations: w -> [ 0.20225819  0.00062998 -0.00041101  0.00149158]      b -> 2.874960424504719e-06.       cost -> 696.5394479662783\n",
            "After 60 iterations: w -> [ 0.20227654  0.00066292 -0.00052586  0.00076759]      b -> -2.121398435016689e-05.       cost -> 696.4315810903236\n",
            "After 70 iterations: w -> [ 2.02294886e-01  6.95861673e-04 -6.40698346e-04  4.39339840e-05]      b -> -4.5291538033855096e-05.       cost -> 696.3238088910889\n",
            "After 80 iterations: w -> [ 0.20231322  0.0007288  -0.00075552 -0.0006794 ]      b -> -6.935770569154542e-05.       cost -> 696.2161312844124\n",
            "After 90 iterations: w -> [ 0.20233155  0.00076173 -0.00087032 -0.00140242]      b -> -9.341249238597147e-05.       cost -> 696.1085481862066\n",
            "After 100 iterations: w -> [ 0.20234987  0.00079467 -0.0009851  -0.00212511]      b -> -0.00011745590317761633.       cost -> 696.0010595124643\n",
            "After 110 iterations: w -> [ 0.20236819  0.0008276  -0.00109987 -0.00284747]      b -> -0.00014148794312471313.       cost -> 695.8936651792471\n",
            "After 120 iterations: w -> [ 0.20238649  0.00086052 -0.00121462 -0.00356952]      b -> -0.00016550861728324625.       cost -> 695.7863651026965\n",
            "After 130 iterations: w -> [ 0.20240479  0.00089345 -0.00132935 -0.00429124]      b -> -0.00018951793070695247.       cost -> 695.6791591990242\n",
            "After 140 iterations: w -> [ 0.20242308  0.00092637 -0.00144407 -0.00501264]      b -> -0.00021351588844732168.       cost -> 695.5720473845181\n",
            "After 150 iterations: w -> [ 0.20244136  0.00095929 -0.00155877 -0.00573372]      b -> -0.00023750249555359802.       cost -> 695.4650295755393\n",
            "After 160 iterations: w -> [ 0.20245963  0.00099221 -0.00167346 -0.00645447]      b -> -0.00026147775707278096.       cost -> 695.3581056885309\n",
            "After 170 iterations: w -> [ 0.2024779   0.00102513 -0.00178813 -0.0071749 ]      b -> -0.00028544167804962593.       cost -> 695.2512756399949\n",
            "After 180 iterations: w -> [ 0.20249615  0.00105804 -0.00190278 -0.00789501]      b -> -0.0003093942635266461.       cost -> 695.1445393465234\n",
            "After 190 iterations: w -> [ 0.2025144   0.00109095 -0.00201741 -0.0086148 ]      b -> -0.00033333551854411236.       cost -> 695.037896724772\n",
            "After 200 iterations: w -> [ 0.20253264  0.00112386 -0.00213203 -0.00933426]      b -> -0.00035726544814005507.       cost -> 694.931347691476\n",
            "After 210 iterations: w -> [ 0.20255087  0.00115677 -0.00224663 -0.01005341]      b -> -0.00038118405735026463.       cost -> 694.8248921634419\n",
            "After 220 iterations: w -> [ 0.20256909  0.00118967 -0.00236122 -0.01077223]      b -> -0.0004050913512082932.       cost -> 694.71853005755\n",
            "After 230 iterations: w -> [ 0.20258731  0.00122257 -0.00247579 -0.01149073]      b -> -0.00042898733474545453.       cost -> 694.6122612907579\n",
            "After 240 iterations: w -> [ 0.20260552  0.00125547 -0.00259034 -0.01220891]      b -> -0.00045287201299082575.       cost -> 694.5060857800959\n",
            "After 250 iterations: w -> [ 0.20262371  0.00128837 -0.00270488 -0.01292677]      b -> -0.0004767453909712485.       cost -> 694.4000034426629\n",
            "After 260 iterations: w -> [ 0.2026419   0.00132127 -0.0028194  -0.01364431]      b -> -0.0005006074737113291.       cost -> 694.2940141956369\n",
            "After 270 iterations: w -> [ 0.20266009  0.00135416 -0.0029339  -0.01436153]      b -> -0.0005244582662334407.       cost -> 694.188117956269\n",
            "After 280 iterations: w -> [ 0.20267826  0.00138705 -0.00304839 -0.01507842]      b -> -0.0005482977735577229.       cost -> 694.0823146418841\n",
            "After 290 iterations: w -> [ 0.20269643  0.00141994 -0.00316286 -0.015795  ]      b -> -0.000572126000702084.       cost -> 693.9766041698765\n",
            "After 300 iterations: w -> [ 0.20271459  0.00145282 -0.00327731 -0.01651125]      b -> -0.0005959429526822015.       cost -> 693.8709864577195\n",
            "After 310 iterations: w -> [ 0.20273274  0.00148571 -0.00339175 -0.01722719]      b -> -0.0006197486345115225.       cost -> 693.7654614229534\n",
            "After 320 iterations: w -> [ 0.20275088  0.00151859 -0.00350617 -0.0179428 ]      b -> -0.0006435430512012659.       cost -> 693.6600289832008\n",
            "After 330 iterations: w -> [ 0.20276901  0.00155147 -0.00362058 -0.0186581 ]      b -> -0.0006673262077604224.       cost -> 693.5546890561482\n",
            "After 340 iterations: w -> [ 0.20278714  0.00158435 -0.00373497 -0.01937307]      b -> -0.0006910981091957559.       cost -> 693.4494415595605\n",
            "After 350 iterations: w -> [ 0.20280526  0.00161722 -0.00384934 -0.02008773]      b -> -0.0007148587605118045.       cost -> 693.3442864112783\n",
            "After 360 iterations: w -> [ 0.20282336  0.00165009 -0.00396369 -0.02080207]      b -> -0.0007386081667108816.       cost -> 693.2392235292045\n",
            "After 370 iterations: w -> [ 0.20284147  0.00168296 -0.00407803 -0.02151608]      b -> -0.0007623463327930761.       cost -> 693.1342528313266\n",
            "After 380 iterations: w -> [ 0.20285956  0.00171583 -0.00419236 -0.02222978]      b -> -0.0007860732637562543.       cost -> 693.0293742356982\n",
            "After 390 iterations: w -> [ 0.20287764  0.0017487  -0.00430666 -0.02294316]      b -> -0.0008097889645960608.       cost -> 692.9245876604514\n",
            "After 400 iterations: w -> [ 0.20289572  0.00178156 -0.00442095 -0.02365622]      b -> -0.0008334934403059189.       cost -> 692.8198930237816\n",
            "After 410 iterations: w -> [ 0.20291379  0.00181442 -0.00453523 -0.02436896]      b -> -0.0008571866958770328.       cost -> 692.7152902439693\n",
            "After 420 iterations: w -> [ 0.20293185  0.00184728 -0.00464948 -0.02508138]      b -> -0.0008808687362983863.       cost -> 692.6107792393573\n",
            "After 430 iterations: w -> [ 0.20294991  0.00188013 -0.00476373 -0.02579348]      b -> -0.0009045395665567463.       cost -> 692.5063599283673\n",
            "After 440 iterations: w -> [ 0.20296795  0.00191299 -0.00487795 -0.02650527]      b -> -0.0009281991916366625.       cost -> 692.4020322294887\n",
            "After 450 iterations: w -> [ 0.20298599  0.00194584 -0.00499216 -0.02721674]      b -> -0.0009518476165204685.       cost -> 692.2977960612889\n",
            "After 460 iterations: w -> [ 0.20300402  0.00197869 -0.00510635 -0.02792788]      b -> -0.0009754848461882827.       cost -> 692.1936513424012\n",
            "After 470 iterations: w -> [ 0.20302204  0.00201154 -0.00522053 -0.02863872]      b -> -0.0009991108856180102.       cost -> 692.0895979915376\n",
            "After 480 iterations: w -> [ 0.20304005  0.00204438 -0.00533469 -0.02934923]      b -> -0.0010227257397853423.       cost -> 691.9856359274768\n",
            "After 490 iterations: w -> [ 0.20305805  0.00207722 -0.00544883 -0.03005942]      b -> -0.0010463294136637588.       cost -> 691.8817650690744\n",
            "After 500 iterations: w -> [ 0.20307605  0.00211006 -0.00556296 -0.0307693 ]      b -> -0.001069921912224527.       cost -> 691.7779853352547\n",
            "After 510 iterations: w -> [ 0.20309404  0.0021429  -0.00567707 -0.03147886]      b -> -0.0010935032404367064.       cost -> 691.674296645017\n",
            "After 520 iterations: w -> [ 0.20311202  0.00217574 -0.00579116 -0.0321881 ]      b -> -0.001117073403267146.       cost -> 691.5706989174297\n",
            "After 530 iterations: w -> [ 0.20312999  0.00220857 -0.00590524 -0.03289703]      b -> -0.001140632405680486.       cost -> 691.4671920716332\n",
            "After 540 iterations: w -> [ 0.20314796  0.0022414  -0.0060193  -0.03360564]      b -> -0.0011641802526391617.       cost -> 691.3637760268448\n",
            "After 550 iterations: w -> [ 0.20316591  0.00227423 -0.00613335 -0.03431393]      b -> -0.0011877169491034.       cost -> 691.2604507023472\n",
            "After 560 iterations: w -> [ 0.20318386  0.00230706 -0.00624738 -0.03502191]      b -> -0.0012112425000312232.       cost -> 691.1572160174996\n",
            "After 570 iterations: w -> [ 0.2032018   0.00233988 -0.00636139 -0.03572956]      b -> -0.0012347569103784502.       cost -> 691.0540718917259\n",
            "After 580 iterations: w -> [ 0.20321973  0.0023727  -0.00647539 -0.03643691]      b -> -0.0012582601850986955.       cost -> 690.9510182445324\n",
            "After 590 iterations: w -> [ 0.20323766  0.00240552 -0.00658937 -0.03714393]      b -> -0.0012817523291433726.       cost -> 690.8480549954894\n",
            "After 600 iterations: w -> [ 0.20325557  0.00243834 -0.00670333 -0.03785064]      b -> -0.0013052333474616931.       cost -> 690.7451820642369\n",
            "After 610 iterations: w -> [ 0.20327348  0.00247116 -0.00681728 -0.03855704]      b -> -0.0013287032450006685.       cost -> 690.6423993704946\n",
            "After 620 iterations: w -> [ 0.20329138  0.00250397 -0.00693121 -0.03926311]      b -> -0.0013521620267051104.       cost -> 690.5397068340459\n",
            "After 630 iterations: w -> [ 0.20330927  0.00253678 -0.00704513 -0.03996888]      b -> -0.0013756096975176325.       cost -> 690.4371043747489\n",
            "After 640 iterations: w -> [ 0.20332716  0.00256959 -0.00715903 -0.04067432]      b -> -0.001399046262378651.       cost -> 690.3345919125309\n",
            "After 650 iterations: w -> [ 0.20334503  0.00260239 -0.00727291 -0.04137945]      b -> -0.0014224717262263863.       cost -> 690.2321693673966\n",
            "After 660 iterations: w -> [ 0.2033629   0.0026352  -0.00738678 -0.04208427]      b -> -0.0014458860939968628.       cost -> 690.1298366594126\n",
            "After 670 iterations: w -> [ 0.20338076  0.002668   -0.00750063 -0.04278877]      b -> -0.0014692893706239104.       cost -> 690.0275937087223\n",
            "After 680 iterations: w -> [ 0.20339861  0.0027008  -0.00761447 -0.04349296]      b -> -0.0014926815610391655.       cost -> 689.9254404355418\n",
            "After 690 iterations: w -> [ 0.20341646  0.0027336  -0.00772829 -0.04419683]      b -> -0.0015160626701720727.       cost -> 689.8233767601507\n",
            "After 700 iterations: w -> [ 0.20343429  0.00276639 -0.00784209 -0.04490038]      b -> -0.0015394327029498848.       cost -> 689.7214026029069\n",
            "After 710 iterations: w -> [ 0.20345212  0.00279918 -0.00795588 -0.04560362]      b -> -0.0015627916642976635.       cost -> 689.6195178842355\n",
            "After 720 iterations: w -> [ 0.20346994  0.00283197 -0.00806965 -0.04630655]      b -> -0.0015861395591382818.       cost -> 689.5177225246308\n",
            "After 730 iterations: w -> [ 0.20348775  0.00286476 -0.0081834  -0.04700916]      b -> -0.0016094763923924238.       cost -> 689.4160164446637\n",
            "After 740 iterations: w -> [ 0.20350556  0.00289755 -0.00829714 -0.04771146]      b -> -0.001632802168978585.       cost -> 689.3143995649705\n",
            "After 750 iterations: w -> [ 0.20352336  0.00293033 -0.00841086 -0.04841345]      b -> -0.0016561168938130763.       cost -> 689.2128718062581\n",
            "After 760 iterations: w -> [ 0.20354114  0.00296311 -0.00852457 -0.04911512]      b -> -0.0016794205718100215.       cost -> 689.1114330893063\n",
            "After 770 iterations: w -> [ 0.20355892  0.00299589 -0.00863826 -0.04981647]      b -> -0.00170271320788136.       cost -> 689.010083334964\n",
            "After 780 iterations: w -> [ 0.2035767   0.00302867 -0.00875193 -0.05051752]      b -> -0.0017259948069368476.       cost -> 688.9088224641534\n",
            "After 790 iterations: w -> [ 0.20359446  0.00306144 -0.00886559 -0.05121825]      b -> -0.0017492653738840572.       cost -> 688.8076503978587\n",
            "After 800 iterations: w -> [ 0.20361222  0.00309422 -0.00897923 -0.05191866]      b -> -0.00177252491362838.       cost -> 688.706567057147\n",
            "After 810 iterations: w -> [ 0.20362997  0.00312699 -0.00909286 -0.05261877]      b -> -0.001795773431073026.       cost -> 688.6055723631445\n",
            "After 820 iterations: w -> [ 0.20364771  0.00315975 -0.00920647 -0.05331856]      b -> -0.0018190109311190263.       cost -> 688.5046662370511\n",
            "After 830 iterations: w -> [ 0.20366544  0.00319252 -0.00932006 -0.05401803]      b -> -0.0018422374186652319.       cost -> 688.4038486001416\n",
            "After 840 iterations: w -> [ 0.20368317  0.00322528 -0.00943364 -0.0547172 ]      b -> -0.0018654528986083172.       cost -> 688.3031193737498\n",
            "After 850 iterations: w -> [ 0.20370088  0.00325804 -0.0095472  -0.05541605]      b -> -0.001888657375842779.       cost -> 688.2024784792902\n",
            "After 860 iterations: w -> [ 0.20371859  0.0032908  -0.00966075 -0.05611459]      b -> -0.001911850855260938.       cost -> 688.1019258382435\n",
            "After 870 iterations: w -> [ 0.20373629  0.00332356 -0.00977427 -0.05681281]      b -> -0.0019350333417529394.       cost -> 688.0014613721601\n",
            "After 880 iterations: w -> [ 0.20375399  0.00335631 -0.00988779 -0.05751073]      b -> -0.0019582048402067556.       cost -> 687.9010850026552\n",
            "After 890 iterations: w -> [ 0.20377167  0.00338907 -0.01000129 -0.05820833]      b -> -0.0019813653555081864.       cost -> 687.8007966514247\n",
            "After 900 iterations: w -> [ 0.20378935  0.00342182 -0.01011477 -0.05890562]      b -> -0.0020045148925408574.       cost -> 687.7005962402227\n",
            "After 910 iterations: w -> [ 0.20380702  0.00345456 -0.01022823 -0.0596026 ]      b -> -0.0020276534561862237.       cost -> 687.6004836908793\n",
            "After 920 iterations: w -> [ 0.20382468  0.00348731 -0.01034168 -0.06029927]      b -> -0.002050781051323572.       cost -> 687.5004589252936\n",
            "After 930 iterations: w -> [ 0.20384233  0.00352005 -0.01045512 -0.06099562]      b -> -0.002073897682830017.       cost -> 687.400521865433\n",
            "After 940 iterations: w -> [ 0.20385998  0.00355279 -0.01056853 -0.06169167]      b -> -0.0020970033555805074.       cost -> 687.3006724333336\n",
            "After 950 iterations: w -> [ 0.20387762  0.00358553 -0.01068194 -0.0623874 ]      b -> -0.002120098074447824.       cost -> 687.200910551102\n",
            "After 960 iterations: w -> [ 0.20389525  0.00361827 -0.01079532 -0.06308282]      b -> -0.002143181844302582.       cost -> 687.1012361409126\n",
            "After 970 iterations: w -> [ 0.20391287  0.003651   -0.01090869 -0.06377793]      b -> -0.0021662546700132283.       cost -> 687.0016491250133\n",
            "After 980 iterations: w -> [ 0.20393048  0.00368373 -0.01102204 -0.06447273]      b -> -0.0021893165564460487.       cost -> 686.9021494257131\n",
            "After 990 iterations: w -> [ 0.20394809  0.00371646 -0.01113538 -0.06516722]      b -> -0.0022123675084651647.       cost -> 686.8027369653997\n",
            "After 1000 iterations: w -> [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ]      b -> -0.002235407530932535.       cost -> 686.7034116665205\n"
          ]
        }
      ],
      "source": [
        "# Training data\n",
        "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "model = MultipleLinearRegression(x_train, y_train)\n",
        "\n",
        "\n",
        "# Perform gradient descent to find the optimal weight and bias\n",
        "w, b = model.gradient_descent(iterations=1000, learning_rate=5.0e-7)\n",
        "\n",
        "# # Scatter plot of the training data\n",
        "# plt.scatter(x_train, y_train, marker = \"x\", color = \"red\")\n",
        "\n",
        "# # Display the final optimized weight and bias\n",
        "# print(f\"w -> {w}.     b -> {b}\")\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UewdIci2yr47"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2104    5    1   45]\n"
          ]
        }
      ],
      "source": [
        "x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
        "y_train = np.array([460, 232, 178])\n",
        "\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
